---
title: "GetBaltimoreCamera"
author: "tedscott"
date: "December 13, 2015"
output: html_document
---

Example of how to use the first few lines to determine classes and speed up actual reading
# of the full file


```{r}
# 
# use top 50 rows to determine classes of columns, then call read.table using those classes
# to improve the speed of file reading
initial <- read.table("input.txt",nrows=50)
classes <- sapply(initial,class)
fullDF <- read.table("input.txt",colClasses = classes)

```



How to read in a csv from the interwebs

```{r}
getwd()
# could check for a directory called data with
# if(!dir.exists("data")){dir.create("data")}
# if we want the read file to be in a data directory
fileURL <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileURL, destfile="./camera.csv")
# download.file(fileURL, destfile="./data/camera.csv")

# note if it is an excel file instead of a csv then do
# library(xslx)
# cameras <- read.xlsx("filename.xlsx", sheetIndex=1, header=TRUE)
# can also set colIndex and rowIndex parameters to the range of columns and rows you want to read

list.files()
dateDownloaded <- date()
dateDownloaded
cameras <- read.csv("./camera.csv")
# OR can do: read.table("./camera.csv", sep=",", header=TRUE, quote="", na.strings="NA")
summary(cameras)
```

How to read in an XML file from the interwebs

```{r}
library(XML)
fileURL <- "http://www.w3schools.com/xml/simple/xml"
doc <- xmlTreeParse(fileURL, useInternal=TRUE)
rootNode <- xmlRoot(doc)
# get name of root node
xmlName(rootNode)
# dump out names of all next level elements under root node 
names(rootNode)

# what is in the first node? list-like notation
rootNode[[1]]

# what is the first element of the first node?
rootNode[[1]][[1]]

# dump out all values using an sapply like func with xmlValue function
xmlSApply(rootNode, xmlValue)

# how about just the names?
# use xpath functions
xpathSApply(rootNode, "//name", xmlValue)

# or prices
xpathSApply(rootNode,"//price",xmlValue)

#
# extract contents of Baltimore Ravens website on ESPN
# using html parsing functions
fileURL <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileURL, useInternal=TRUE)
# can go to that page above and manually view the source in the browser to see the elements used below
# find list elements (li) with the class=score and team-name
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
scores
teams

```


How about JSON?
```{r}
# example JSON https://api.github.com/uers/jtleek/repos (view the source?)
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/uers/jtleek/repos")
# dump out all the names of nodes
names(jsonData)

# find owners
names(jsonData$owner)

# go into owners and find logins
# like a dataframe within a dataframe
jsonData$owner$login

# can also write dataframe to json
myjson <- toJSON(iris, pretty=TRUE)

# show us what it looks like
cat(myjson)

# now make it back into a DF
iris2 <- fromJSON(myjson)
head(iris2)


```


You can also embed plots, for example:

```{r, echo=FALSE}
#plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
