---
title: "Getting & Cleaning Week 4"
author: "tedscott"
date: "January 23, 2016"
output: html_document
---

Editing Text Variables

```{r}

# fixing character vectors tolower() and toupper()
cameraData <- read.csv("./data/camera.csv")
names(cameraData)
tolower(names(cameraData))

# strsplit()
# have to escape the . with \\
splitNames <- strsplit(names(cameraData),"\\.")
splitNames[[5]]
splitNames[[6]]

# now we would want to drop that part of the list we split off
# slight digression on lists
mylist <- list(letters=c("A","b","c"), numbers = 1:3, matrix(1:25, ncol=5))
head(mylist)
mylist[1]

# these are equivalent
mylist$letters
mylist[[1]]

# fixing with sapply()
# this will give us jut the "location" and not the "1"
splitNames[[6]][1]

# define a simple function to clean it up
firstElement <- function(x){x[1]}
sapply(splitNames, firstElement)


# work with peer review data again
reviews <- read.csv("./data/reviews.csv"); solutions <- read.csv("./data/solutions.csv")
head(reviews, 2)
head(solutions, 2)

# revmoe underscores in names with sub()
names(reviews)
sub("_","",names(reviews))

# gsub() would fix all the underscores in the names if there were more than one
testName <- "this_is_a_test"
sub("_","",testName)
gsub("_","",testName)

# searching for values with grep() and grepl()
grep("Alameda", cameraData$intersection)
# appears in 4th, 5th and 36th element

# grepl() will return a logical vector of where it appears
# can create a table to get counts
table(grepl("Alameda", cameraData$intersection))

# can use this to strip out parts of the data
cameraData2 <- cameraData[!grepl("Alameda",cameraData$intersection),]

# if you include value=TRUE in grep() it will return the strings
grep("Alameda",cameraData$intersection, value=TRUE)

# if it is nont found you get 0 back
grep("JeffStreet", cameraData$intersection)

# other useful string functions in stringr package
library(stringr)
nchar("Jeffrey Leek")
substr("Jeffrey Leek", 1, 7)

# this pastes two strings together with a default sep=" "
paste("Jeffrey","Leek")

# paste0 doesn't add a space
paste0("Jeffrey","Leek")

# trim spaces
str_trim("   Jeff        ")


```

Regular Expressions

```{r, echo=FALSE}
# ^ matches beginning of a line, e.g. ^today woudl match all lines starting with "today"
# $ matches the end, so today$ matches all lines that end with today
# character classes [Bb][Uu][Ss][Hh] would match "Bush" "bush" "bushwacking', etc

# can combine
# ~[Ii] am would match sentences beginning with "i am" and "I am"

# matching ranges
# ^[0-9][a-zA-Z] matches lines beginning with a number and any characters of any case after it

# used in the beginning of a character class, the ^ means NOT
# [^?.]$ matches lines that do NOT end in "?" or "."

# . means any character so
# 9.11 means match lines with a 9 followed by one char followed by 11

# | means OR, so flood|fire matches lines with either or both words

# ^[Gg]ood|[Bb]ad matches lines that start with "Good" or "good"
# OR contain "Bad" or "bad"

# constrain with ()
# ^([Gg]ood|[Bb]ad) means it has to start with good or bad

# if a ? follows an expression it means that part is optional
# [Gg]eorge([Ww]\.)? [Bb]ush matches "George Bush" and "george w. bush"
# note that the "." had to be escaped as \. since it is a special char

# * means repeated any number of times
# (.*) means something in () with any number of characters including none
# + means at least one of the item
# [0-9]+(.*)[0-9]+ means at least one number followed by any number of
# characters followed by at least one number

# {} give interval qualifiers, or the min and max number of matches of 
# an expression
# [Bb]ush(+[^ ]+ +){1,5} debate
# matches "Bush" followed by at least one space
# followed by something that is not a space
# followed by a space
# between 1 and 5 times
# then "debate"
# e.g. "Bush has historically won all major debates he's done"

# {m,n} means at least m but not more than n matches
# {m} means exactly m matches
# {m,]} means at least m matches

# \1 and \2, etc refer to the match expression, in order that was in ()
# so
# +([a-zA-Z]+) +\1 +
# matches 
# at least one letter followed by a space followed by at least one letter
# again (but the same one as before) followed by a space
# e.g. it looks for repeated words
# so it would match
# "time for bed, night night twitter"
# "blah blah blah blah"

# the * is greedy so it matches the longest possible string that satisfies
# the expression
# ^s(.*)s matches any line that starts with an s and then has an s later
# e.g. "sitting at starbucks"
# "studying stuff for the exams"

# grrediness can be turned off with ?
# ^s(.*?)s$


```

Working with Dates

```{r}

# date() gives current date/time
d1=date()
d1

# note it is of type character
class(d1)

# Sys.date()
d2 <- Sys.Date()
d2
# now type is Date
class(d2)

# formatting dates
# %d = day as number (0-31)
# %a = abbreviated weekday
# %A = unabbreviated weekday
# %m = month as number (01-12)
# %b = abbrev month
# %B = unabbrev month
# %y = 2-digit year
# %Y = 4-digit year

# use format()
format(d2, "%a %b %d")

# creating dates
x <- c("1jan1960","2jan1960","30jul1960")
z <- as.Date(x, "%d%b%Y")
z

# can do simple arithmetic with dates
z[1] - z[2]

# or get integers
as.numeric(z[1] - z[2])

# can get nicer printing
weekdays(d2)
months(d2)
months(z)

# convert to Julian - number of days since 1970-01-01
julian(d2)


# use lubridate package to make dates easier to deal with
install.packages("lubridate")
library(lubridate)

ymd("20140108")
mdy("08/04/2013")
dmy("03-04-2013")

# see what class they are
d3 <- mdy("08/04/2013")
class(d3)

# it is POSIXct POSIXt


# now dealing with TIME
ymd_hms("2011-08-03 10:15:03")

ymd_hms("2011-08-03 10:15:03", tz="Pacific/Auckland")

# what is my TZ?
?Sys.timezone
Sys.timezone()

# syntax - extract weekday number
x <- dmy(c("1jan1960","2jan1960","30jul1960"))
wday(x[1])

# get the label for that day
wday(x[1], label = TRUE)

# want date objects to be class Date or POSIXlt
# learn more with 
?POSIXlt



```

Data Resources

http://www.data.gov and links from it

development & health: http://www.gapminder.org

survey data from US: http://www.asdfree.com

infochimps
kaggle

http://blog.mortardata.com/post/67652898761/6-dataset-lists-curated-by-data-scientists

APIs
twitteR package makes it easier than creating an app
RFacebook
RGoogleMaps
